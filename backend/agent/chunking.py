import os
import tempfile
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
import sys  
from pathlib import Path


def extract_text_from_document(file_path: str) -> str:
    """
    Azure Document Intelligenceë¥¼ í™œìš©í•´ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Args:
        file_path (str): ë¶„ì„í•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
    Returns:
        str: ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸
    """
    endpoint = os.getenv("AZURE_DI_ENDPOINT")
    key = os.getenv("AZURE_DI_KEY")
    client = DocumentAnalysisClient(endpoint, AzureKeyCredential(key))

    with open(file_path, "rb") as f:
        poller = client.begin_analyze_document("prebuilt-document", document=f, pages="1-9")
        result = poller.result()

    # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹¨
    full_text = ""
    for page in result.pages:
        for line in page.lines:
            full_text += line.content + "\n"
    return full_text

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ chunk_size ë‹¨ìœ„ë¡œ ê²¹ì¹˜ê²Œ ë¶„í• í•©ë‹ˆë‹¤.
    Args:
        text (str): ì „ì²´ í…ìŠ¤íŠ¸
        chunk_size (int): ì²­í¬ í¬ê¸°
        overlap (int): ì²­í¬ ê°„ ê²¹ì¹¨ ê¸¸ì´
    Returns:
        list[str]: ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

def save_chunks_to_temp_file(chunks):
    """
    ì²­í‚¹ëœ í…ìŠ¤íŠ¸ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    Args:
        chunks (list): ì²­í‚¹ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    Returns:
        str: ì„ì‹œ íŒŒì¼ ê²½ë¡œ
    """
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')
    temp_file_path = temp_file.name
    
    try:
        for i, chunk in enumerate(chunks):
            temp_file.write(f"--- Chunk {i+1} ---\n{chunk}\n\n")
        temp_file.close()
        print(f"chunking.py: âœ… ì²­í‚¹ ê²°ê³¼ê°€ ì„ì‹œ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {temp_file_path}")
        return temp_file_path
    except Exception as e:
        print(f"chunking.py: âŒ ì„ì‹œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        if os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
        raise

def process_document_to_temp_file(file_path: str, chunk_size: int = 1000, overlap: int = 200):
    """
    ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ê³  ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    Args:
        file_path (str): ë¶„ì„í•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
        chunk_size (int): ì²­í¬ í¬ê¸°
        overlap (int): ì²­í¬ ê°„ ê²¹ì¹¨ ê¸¸ì´
    Returns:
        str: ì„ì‹œ íŒŒì¼ ê²½ë¡œ
    """
    print(f"chunking.py: ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
    
    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print("chunking.py: ğŸ” í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    text = extract_text_from_document(file_path)
    print(f"chunking.py: âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ì´ {len(text)} ë¬¸ì)")
    
    # 2. ì²­í‚¹
    print("chunking.py: âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
    chunks = chunk_text(text, chunk_size, overlap)
    print(f"chunking.py: âœ… ì²­í‚¹ ì™„ë£Œ (ì´ {len(chunks)}ê°œ ì²­í¬)")
    
    # 3. ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    print("chunking.py: ğŸ’¾ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    temp_file_path = save_chunks_to_temp_file(chunks)
    
    print(f"chunking.py: ğŸ‰ ì²­í‚¹ ì™„ë£Œ! ì„ì‹œ íŒŒì¼: {temp_file_path}")
    return temp_file_path

def extract_texts_by_page(file_path: str):
    """
    Azure Document Intelligenceë¥¼ í™œìš©í•´ ë¬¸ì„œì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Returns:
        list[tuple(page_text:str, page_num:int)]
    """
    endpoint = os.getenv("AZURE_DI_ENDPOINT")
    key = os.getenv("AZURE_DI_KEY")
    client = DocumentAnalysisClient(endpoint, AzureKeyCredential(key))

    with open(file_path, "rb") as f:
        poller = client.begin_analyze_document("prebuilt-document", document=f, pages="1-9")
        result = poller.result()

    page_texts = []
    for idx, page in enumerate(result.pages):
        page_text = ""
        for line in page.lines:
            page_text += line.content + "\n"
        page_texts.append((page_text, idx+1))
    return page_texts

def chunk_by_paragraph_with_page_range(file_path: str, min_chunk_len: int = 200):
    """
    í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ë’¤, ì˜ë¯¸ ë‹¨ìœ„(ë¬¸ë‹¨)ë¡œ ì²­í‚¹í•˜ê³ ,
    ê° ì²­í¬ê°€ ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì¹˜ë©´ page='1~2'ì™€ ê°™ì´ ë²”ìœ„ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    Returns:
        list[tuple(chunk_text:str, page_range:str)]
    """
    page_texts = extract_texts_by_page(file_path)
    paragraphs = []
    page_marks = []
    for text, page_num in page_texts:
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°
        for para in text.split("\n\n"):
            para = para.strip()
            if para:
                paragraphs.append(para)
                page_marks.append(page_num)
    # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¶ê¸° (min_chunk_len ì´ìƒì´ ë  ë•Œê¹Œì§€ ë¬¸ë‹¨ í•©ì¹˜ê¸°)
    chunks = []
    i = 0
    while i < len(paragraphs):
        chunk = paragraphs[i]
        start_page = page_marks[i]
        end_page = start_page
        j = i + 1
        while len(chunk) < min_chunk_len and j < len(paragraphs):
            chunk += '\n\n' + paragraphs[j]
            end_page = page_marks[j]
            j += 1
        if start_page == end_page:
            page_range = str(start_page)
        else:
            page_range = f"{start_page}~{end_page}"
        chunks.append((chunk, page_range))
        i = j
    return chunks

if __name__ == "__main__":
    # sub_lang/agent/chunking.pyì—ì„œ sub_lang/docs/ë¡œ ê°€ëŠ” ìƒëŒ€ê²½ë¡œ
    file_path = "../docs/Customer Service_Booking_Manual.pdf"
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path):
        print(f"chunking.py: âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        print(f"chunking.py: í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"chunking.py: ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(file_path)}")
        exit(1)
    
    print(f"chunking.py: âœ… íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {file_path}")
    
    # ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ì‹œ íŒŒì¼ ì €ì¥
    temp_file_path = process_document_to_temp_file(file_path, chunk_size=1000, overlap=200)
    
    print(f"chunking.py: 
ğŸ“ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„ë² ë”©ì„ ì§„í–‰í•˜ì„¸ìš”:")
    print(f"chunking.py: python sub_lang/agent/embedding.py {temp_file_path}")