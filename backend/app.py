import os
import sys
from pathlib import Path

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))



from logger_config import setup_logging
import logging
setup_logging()
logger = logging.getLogger(__name__)

from agent.rag_agent import handle_rag
import re
import openpyxl
import json
import pandas as pd
import uvicorn
import random
import ast
import auto_generator.sentence_generator
import auto_generator.pdf_question_generator
from starlette.middleware.sessions import SessionMiddleware
from fastapi import FastAPI, File, Form, UploadFile, HTTPException, Request, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.responses import FileResponse
from typing import Union
from tempfile import NamedTemporaryFile
from enum import Enum
from pydantic import BaseModel

from agent.document_processing.processing_pdf import process_files_in_directory, batch_process_directory, merge_adjacent_table, parse_table_for_embedding, replace_markdown_with_llm_data, divide_large_passage
from agent.embedding import process_chunks, process_chunks_with_metadata

def detect_table_content(content_str: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ í…Œì´ë¸” ë‚´ìš©ì¸ì§€ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    """
    # í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ íŒ¨í„´
    table_patterns = [
        r'\|[^|]+\|[^|]+\|',  # | ì»¬ëŸ¼1 | ì»¬ëŸ¼2 |
        r'[A-Za-z]+\s*:\s*[A-Za-z0-9\s]+,\s*[A-Za-z]+\s*:\s*[A-Za-z0-9\s]+',  # ì»¬ëŸ¼1: ê°’1, ì»¬ëŸ¼2: ê°’2
        r'^\s*[A-Za-z]+\s*:\s*[A-Za-z0-9\s]+\s*$',  # ë‹¨ì¼ ì»¬ëŸ¼: ê°’
    ]
    
    for pattern in table_patterns:
        if re.search(pattern, content_str, re.MULTILINE):
            return True
    
    return False

def calculate_table_match_score(content_str: str, table_data: dict) -> int:
    """
    í…Œì´ë¸” ë§¤ì¹­ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    score = 0
    
    # llm_generate_data ë§¤ì¹­
    if table_data.get('llm_generate_data'):
        for llm_item in table_data['llm_generate_data']:
            if llm_item in content_str:
                score += len(llm_item) * 2  # ë” ê¸´ ë§¤ì¹­ì— ë” ë†’ì€ ì ìˆ˜
    
    # í…Œì´ë¸” êµ¬ì¡° íŒ¨í„´ ë§¤ì¹­
    table_pattern = r'\|[^|]+\|[^|]+\|'
    if re.search(table_pattern, content_str):
        score += 50
    
    # í˜ì´ì§€ ë²ˆí˜¸ ì¼ì¹˜ (ì •ìˆ˜ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ)
    if 'page' in table_data and str(table_data['page']) in content_str:
        score += 100
    
    return score

if __name__ == "__main__" :
    uvicorn.run("app:app", host = "0.0.0.0", port=8000, reload = True)

app = FastAPI()

# CORS í—ˆìš© (í”„ë¡ íŠ¸ì—”ë“œì™€ì˜ í†µì‹ ì„ ìœ„í•´)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ì„œë¹„ìŠ¤ ì‹œì—ëŠ” ë„ë©”ì¸ ì œí•œ ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    question: str

@app.post("/process-pdf")
async def process_pdf_endpoint(file: UploadFile = File(...)):
    logger.info("âœ… /process-pdf ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ ì‹œì‘")
    try:
        logger.info(f"ğŸ“„ ì—…ë¡œë“œëœ íŒŒì¼ëª…: {file.filename}")
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        logger.info(f"ğŸ’¾ íŒŒì¼ ì„ì‹œ ì €ì¥ ì™„ë£Œ: {temp_file_path}")

        # ì„ì‹œ íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        temp_dir = os.path.dirname(temp_file_path)

        # processing_pdf.pyì˜ í•¨ìˆ˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œ
        logger.info(">> 1. PDF -> Markdown ë³€í™˜ ì‹œì‘")
        results, md_files = process_files_in_directory(temp_dir)
        if not results:
            logger.error("âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: 'results'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            raise HTTPException(status_code=400, detail="PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… 1. PDF -> Markdown ë³€í™˜ ì™„ë£Œ")

        logger.info(">> 2. ë§ˆí¬ë‹¤ìš´ Chunking ì‹œì‘")
        df = batch_process_directory(md_files)
        if df.empty:
            logger.error("âŒ ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬ ì‹¤íŒ¨: ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            raise HTTPException(status_code=400, detail="ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        logger.info(f"âœ… 2. ë§ˆí¬ë‹¤ìš´ Chunking ì™„ë£Œ. {len(df)}ê°œ íŒ¨ì‹œì§€ ìƒì„±.")

        all_tables_data = []
        for result in results:
            all_tables_data.extend(result['tables_data'])

        logger.info(">> 3. í…Œì´ë¸” ë³‘í•© ì‹œì‘")
        df, all_tables_data = merge_adjacent_table(df, all_tables_data)
        logger.info("âœ… 3. í…Œì´ë¸” ë³‘í•© ì™„ë£Œ")

        logger.info(">> 4. í…Œì´ë¸” ë°ì´í„° ë¬¸ì¥ ë³€í™˜ ì‹œì‘")
        all_tables_data = parse_table_for_embedding(all_tables_data)
        df, all_tables_data = replace_markdown_with_llm_data(df, all_tables_data)
        logger.info("âœ… 4. í…Œì´ë¸” ë°ì´í„° ë¬¸ì¥ ë³€í™˜ ì™„ë£Œ")

        logger.info(">> 5. ëŒ€ìš©ëŸ‰ íŒ¨ì‹œì§€ ë¶„í•  ì‹œì‘")
        df = divide_large_passage(df)
        logger.info("âœ… 5. ëŒ€ìš©ëŸ‰ íŒ¨ì‹œì§€ ë¶„í•  ì™„ë£Œ")


        # process_chunks í•¨ìˆ˜ì— ë§ê²Œ ë°ì´í„° ê°€ê³µ (í…Œì´ë¸” ë©”íƒ€ë°ì´í„° í™œìš©)
        logger.info(">> 6. ì„ë² ë”© ë°ì´í„° ê°€ê³µ ì‹œì‘")
        chunks_for_embedding = []
        for _, row in df.iterrows():
            page = row['pages'][0] if row['pages'] else 1
            
            # processing_pdf.pyì—ì„œ ìƒì„±í•œ íŠœí”Œ êµ¬ì¡°ë¥¼ (text, page) í˜•íƒœë¡œ ë³€í™˜
            if isinstance(row['content'], tuple) and len(row['content']) >= 5:
                # íŠœí”Œì¸ ê²½ìš°: (ëŒ€ì œëª©, ì¤‘ì œëª©, ì†Œì œëª©, ì„¸ì œëª©, ë‚´ìš©, ...)
                content_text = row['content'][4]  # 5ë²ˆì§¸ ìš”ì†Œê°€ ì‹¤ì œ ë‚´ìš©
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš°: ê·¸ëŒ€ë¡œ ì‚¬ìš©
                content_text = row['content']
            
            # í…Œì´ë¸” ë°ì´í„°ì¸ì§€ í™•ì¸í•˜ê³  ë©”íƒ€ë°ì´í„° ë§¤ì¹­ (ê°œì„ ëœ ë²„ì „)
            table_metadata = None
            content_str = str(row['content'])
            
            # ë°©ë²• 1: ê°œì„ ëœ ì ìˆ˜ ê¸°ë°˜ ë§¤ì¹­
            best_match_score = 0
            best_match_table = None
            
            # í…Œì´ë¸” ë‚´ìš©ì¸ì§€ ë¨¼ì € ê°ì§€
            is_table_content = detect_table_content(content_str)
            
            for table in all_tables_data:
                match_score = calculate_table_match_score(content_str, table)
                
                # í˜ì´ì§€ ë²ˆí˜¸ ì¼ì¹˜ ì‹œ ì¶”ê°€ ì ìˆ˜
                if table['page'] == page:
                    match_score += 100
                
                # í…Œì´ë¸” ë‚´ìš©ì´ ê°ì§€ëœ ê²½ìš° ì¶”ê°€ ì ìˆ˜
                if is_table_content:
                    match_score += 200
                
                # ë” ë†’ì€ ì ìˆ˜ì˜ ë§¤ì¹­ ì„ íƒ
                if match_score > best_match_score:
                    best_match_score = match_score
                    best_match_table = table
            
            if best_match_table and best_match_score > 100:  # ìµœì†Œ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
                table_metadata = best_match_table
                logger.info(f"ğŸ“Š í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë§¤ì¹­ (ì ìˆ˜ ê¸°ë°˜): {table_metadata['key']} (í˜ì´ì§€: {table_metadata['page']}, ì ìˆ˜: {best_match_score})")
            
            # ë°©ë²• 2: ë°±ì—… ë§¤ì¹­ (í˜ì´ì§€ ê¸°ë°˜)
            if not table_metadata and is_table_content:
                # í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë§¤ì¹­ (í…Œì´ë¸” ë‚´ìš©ì´ ê°ì§€ëœ ê²½ìš°ì—ë§Œ)
                for table in all_tables_data:
                    if table['page'] == page:
                        table_metadata = table
                        logger.info(f"ğŸ“Š í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë§¤ì¹­ (í˜ì´ì§€ ë°±ì—…): {table['key']} (í˜ì´ì§€: {table['page']})")
                        break
            
            # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…Œì´ë¸” ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì €ì¥
            if table_metadata:
                # í…Œì´ë¸” ì¹´í…Œê³ ë¦¬ ìƒì„± (ì˜ˆ: "Table_5x3", "Table_10x4")
                category = f"Table_{table_metadata['rows']}x{table_metadata['columns']}"
                
                # í…Œì´ë¸” í‚¤ë¥¼ í† í”½ìœ¼ë¡œ ì‚¬ìš© (ê¸¸ì´ ì œí•œ ë° ì •ì œ)
                topic = table_metadata['key'][:200] if len(table_metadata['key']) > 200 else table_metadata['key']
                
                # ë³‘í•©ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                if table_metadata.get('merged_indices'):
                    category += "_Merged"
                    topic = f"Merged_{topic}"
                
                # í…Œì´ë¸” ë°ì´í„°ì¸ì§€ í™•ì¸ (llm_generate_dataê°€ ìˆìœ¼ë©´ í…Œì´ë¸”)
                if table_metadata.get('llm_generate_data'):
                    logger.info(f"ğŸ“Š í…Œì´ë¸” ë°ì´í„° ê°ì§€: {category} - {topic}")
                else:
                    logger.info(f"ğŸ“„ ì¼ë°˜ í…ìŠ¤íŠ¸ (í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ìˆìŒ): {category} - {topic}")
                
                chunks_for_embedding.append((content_text, page, category, topic, table_metadata))
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                category = "Text"
                topic = f"Page_{page}"
                logger.info(f"ğŸ“„ ì¼ë°˜ í…ìŠ¤íŠ¸: {category} - {topic}")
                chunks_for_embedding.append((content_text, page, category, topic, None))
                
        logger.info(f"âœ… 6. ì„ë² ë”© ë°ì´í„° ê°€ê³µ ì™„ë£Œ. {len(chunks_for_embedding)}ê°œ ì²­í¬ ì¤€ë¹„.")

        # ì„ë² ë”© ë° ì €ì¥ (í…Œì´ë¸” ë©”íƒ€ë°ì´í„° í¬í•¨)
        logger.info(">> 7. Milvus ì„ë² ë”© ë° ì €ì¥ ì‹œì‘")
        process_chunks_with_metadata(chunks_for_embedding)
        logger.info("âœ… 7. Milvus ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ")


        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_file_path)
        logger.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_file_path}")

        # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆì„ ë•Œ ë°˜í™˜
        logger.info("ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì„±ê³µ! JSON ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return JSONResponse(content={"message": "PDF íŒŒì¼ ì²˜ë¦¬ ë° ì„ë² ë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", "chunk_count": len(df)})

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"â—ï¸â—ï¸â—ï¸ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ â—ï¸â—ï¸â—ï¸\n{error_details}")
        return JSONResponse(
            status_code=500,
            content={"error": "ë°±ì—”ë“œ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ", "details": str(e), "traceback": error_details}
        )



@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    answer = handle_rag(req.question)
    return {"answer": answer}

@app.get("/")
async def root():
    return {"message": "RAG Chatbot API is running!"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "backend"}

# HTML í…œí”Œë¦¿ ì—”ë“œí¬ì¸íŠ¸ë“¤ì€ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì œê±°
# í”„ë¡ íŠ¸ì—”ë“œ: http://localhost:3000 ì—ì„œ ì›¹ í˜ì´ì§€ ì„œë¹™


# í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ Form (PDF, ì´ë¯¸ì§€, XML ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±)
@app.post("/uploadfiles/pdf")
async def create_upload_documents(
    project_id: str = Form(...),
    file1: UploadFile = File(...),
    file2: UploadFile = File(...)
):
    
    async def save_file_to_temp(upload_file: UploadFile):
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        supported_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.xml']
        file_extension = os.path.splitext(upload_file.filename)[1].lower()
        
        if file_extension not in supported_extensions:
            logger.error(f"app.py: File with invalid extension uploaded: {upload_file.filename}")
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}. ì§€ì› í˜•ì‹: {', '.join(supported_extensions)}")
        
        temp_file = NamedTemporaryFile(delete=False, suffix=file_extension)
        with open(temp_file.name, 'wb') as f:
            content = await upload_file.read()
            f.write(content)
        
        logger.info(f"app.py: File saved to temporary location: {temp_file.name}")
        return temp_file.name

    file_path1 = None
    file_path2 = None
    try:
        file_path1 = await save_file_to_temp(file1)
        file_path2 = await save_file_to_temp(file2)

        result = auto_generator.pdf_question_generator.generate_questions_from_documents(
            file_path1=file_path1,
            file_path2=file_path2,
            project_id=project_id
        )

        if not result or 'file_name' not in result:
            logger.error(f"app.py: Question generation failed or returned an invalid result: {result}")
            raise HTTPException(status_code=500, detail="ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"app.py: Successfully generated questions. Result: {result}")
        return result

    except ValueError as e:
        logger.exception("app.py: A value error occurred during document processing")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.exception("app.py: An unexpected error occurred during document processing")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if file_path1 and os.path.exists(file_path1):
            os.remove(file_path1)
            logger.info(f"app.py: Temporary file {file_path1} deleted.")
        if file_path2 and os.path.exists(file_path2):
            os.remove(file_path2)
            logger.info(f"app.py: Temporary file {file_path2} deleted.")

# í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ Form
@app.post("/uploadfiles/sentence")
async def create_upload_files(
    field_business: Union[str, None] = Form(None),
    user_type: Union[str, None] = Form(None),
    Constraints: Union[str, None] = Form(None),
    project_id: str = Form(...),
    file1: UploadFile = File(...),
    file2: UploadFile = File(...)
):
    try:
        # Azure ì„¤ì • ì´ˆê¸°í™”
        auto_generator.sentence_generator.initialize_azure_config(project_id)
        
        # íŒŒì¼ ì €ì¥ í•¨ìˆ˜
        async def save_file_to_temp(upload_file: UploadFile):
            if not upload_file.filename.endswith(".xlsx"):
                logger.error(f"app.py: File with invalid extension uploaded: {upload_file.filename}")
                raise ValueError(f"Invalid file extension for {upload_file.filename}")
            temp_file = NamedTemporaryFile(delete=False, suffix=".xlsx")
            with open(temp_file.name, 'wb') as f:
                while True:
                    chunk = await upload_file.read(1024 * 1024)  # 1MB chunks
                    if not chunk:
                        break
                    f.write(chunk)
            logger.info(f"app.py: File saved to temporary location: {temp_file.name}")
            return temp_file.name

        # Q&A Set(ì§ˆë‹µì…‹) ì €ì¥
        file_path = await save_file_to_temp(file1)
        
        # Entity Dict(ì—”í‹°í‹° ì‚¬ì „) ì €ì¥
        file_path2 = await save_file_to_temp(file2)

        # ë°ì´í„° í”„ë ˆì„ ë¡œë“œ ë° ê²€ì¦
        df = pd.read_excel(file_path)
        edf = pd.read_excel(file_path2)
        
        if (df.columns[0] != 'Answer') and (edf.columns[0] != 'Entry'):
            logger.warning("app.py: File contents do not meet the expected format")
            raise ValueError("File contents do not meet expected format")

        # ë°ì´í„° ì €ì¥ (project_id ì œê±°)
        result = auto_generator.sentence_generator.Save_data(field_business, user_type, df, edf, Constraints)
        logger.info(f"app.py: Save_data result: {result}")
        if not result or 'file_name' not in result:
            logger.error(f"app.py: Returned result does not contain file_name! result: {result}")
        logger.info("app.py: Data successfully saved.")
        return result

    except ValueError as e:
        logger.exception("app.py: A value error occurred")
        return {"error": str(e)}
    except Exception as e:
        logger.exception("app.py: An unexpected error occurred")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if 'file_path' in locals() and os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"app.py: Temporary file {file_path} deleted.")
        if 'file_path2' in locals() and os.path.exists(file_path2):
            os.remove(file_path2)
            logger.info(f"app.py: Temporary file {file_path2} deleted.")


# í•™ìŠµë°ì´í„° ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
@app.get("/downloadfiles/{folder}/{filename}/")
def download_files(folder: str, filename: str):
    logger.info(f"app.py: Download request received for file: {filename} in folder: {folder}")
    
    # í—ˆìš©ëœ í´ë” ëª©ë¡ìœ¼ë¡œ ë³´ì•ˆ ê°•í™”
    allowed_folders = ['sentence', 'pdf_questions']
    if folder not in allowed_folders:
        logger.error(f"app.py: Forbidden folder access attempt: {folder}")
        raise HTTPException(status_code=403, detail="Forbidden folder")

    base_directory = os.getenv('DOWNLOAD_DIRECTORY', f'save_result/{folder}')
    # ê²½ë¡œ ì¡°ì‘ ê³µê²© ë°©ì§€
    safe_filename = os.path.basename(filename)
    file_path = os.path.join(base_directory, safe_filename)
    
    logger.info(f"app.py: Attempting to download file from path: {file_path}")
    
    if not os.path.isfile(file_path):
        # ë§Œì•½ ê¸°ë³¸ ê²½ë¡œì— íŒŒì¼ì´ ì—†ë‹¤ë©´, í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì¬ì‹œë„
        # (save_resultê°€ ì•„ë‹Œ docs/save_result ë“±ì— ì €ì¥ë˜ëŠ” ê²½ìš° ëŒ€ë¹„)
        alt_base_directory = os.path.join('docs', 'save_result', folder)
        file_path = os.path.join(alt_base_directory, safe_filename)
        logger.info(f"app.py: File not in primary path, trying alternative: {file_path}")
        if not os.path.isfile(file_path):
             logger.error(f"app.py: File not found at path: {file_path}")
             raise HTTPException(status_code=404, detail="File not found")

    return FileResponse(file_path, media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', filename=filename)


# ì‚¬ì—…ë¶€ ë“œë¡­ë‹¤ìš´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
@app.get("/get-business-department")
async def get_dropdown_business_department():
    file_path = "/app/docs/public/business_department.xlsx"
    df = pd.read_excel(file_path, sheet_name="Sheet1")

    projects = df[["ProjectID", "ProjectName"]].dropna().to_dict(orient="records")

    return {"options": projects}

# ë¡œê·¸ ë°ì´í„° ë°˜í™˜ API
@app.get("/log")
async def get_logs():
    try:
        log_file_path = "/app/logs/debug.log"
        
        if not os.path.exists(log_file_path):
            logger.warning(f"app.py: Log file not found: {log_file_path}")
            return {"logs": [], "error": "Log file not found"}
        
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()
        
        # ìµœê·¼ 500ì¤„ ë°˜í™˜ (ë” ë§ì€ ë¡œê·¸ í‘œì‹œ)
        recent_logs = log_lines[-500:] if len(log_lines) > 500 else log_lines
        
        # ì‹¤ì œ ë¡œê·¸ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ìš”í•œ ë¡œê·¸ë§Œ í•„í„°ë§)
        valid_logs = []
        for line in recent_logs:
            line = line.strip()
            if line and not line.startswith("returning"):
                # HTTP ë””ë²„ê·¸ ë¡œê·¸ ì œì™¸í•˜ê³  ì¤‘ìš”í•œ ë¡œê·¸ë§Œ í¬í•¨
                if any(keyword in line for keyword in [
                    "INFO", "ERROR", "WARNING", "SUCCESS", "FAILED", 
                    "Milvus", "Azure", "RAG", "Question", "File", "Upload"
                ]):
                    valid_logs.append(line)
        
        # ë¡œê·¸ API í˜¸ì¶œ ë©”ì‹œì§€ëŠ” DEBUG ë ˆë²¨ë¡œ ë³€ê²½ (í™”ë©´ì— í‘œì‹œë˜ì§€ ì•ŠìŒ)
        logger.debug(f"app.py: Log data requested, returning {len(valid_logs)} filtered log lines")
        return {"logs": valid_logs}
        
    except Exception as e:
        logger.error(f"app.py: Error reading log file: {e}")
        return {"logs": [], "error": str(e)}

# mcp ì—°ë™ í…ŒìŠ¤íŠ¸ API
@app.get("/api/mcp")
async def get_mcp_data():
    mcp_file_path = os.path.join(os.path.dirname(__file__), ".gemini", "mcp_test.json")
    try:
        with open(mcp_file_path, 'r', encoding='utf-8') as f:
            mcp_data = json.load(f)
        logger.info(f"app.py: Successfully loaded MCP data from {mcp_file_path}")
        return JSONResponse(content=mcp_data)
    except FileNotFoundError:
        logger.error(f"app.py: MCP data file not found at: {mcp_file_path}")
        raise HTTPException(status_code=404, detail="MCP data file not found")
    except Exception as e:
        logger.exception("app.py: An error occurred while processing MCP data")
        raise HTTPException(status_code=500, detail=str(e))