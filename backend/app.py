import os
import sys
from pathlib import Path

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# BASE_DIR ì •ì˜ (backend root)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

import logging
from logger_config import setup_logging
setup_logging()
logger = logging.getLogger("app")

from agent.rag_agent import handle_rag
import re
import openpyxl
import json
import pandas as pd
import uvicorn
import random
import ast
import io
import auto_generator.sentence_generator
import auto_generator.pdf_question_generator
from starlette.middleware.sessions import SessionMiddleware
from fastapi import FastAPI, File, Form, UploadFile, HTTPException, Request, BackgroundTasks
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.responses import FileResponse
from typing import Union, List
from tempfile import NamedTemporaryFile
from enum import Enum
from pydantic import BaseModel

from agent.document_processing.allegronx_chunker import process_files_in_directory, batch_process_directory, merge_adjacent_table, parse_table_for_embedding, replace_markdown_with_llm_data, divide_large_passage
from agent.embedding import process_chunks, process_chunks_with_metadata
from agent.document_processing.clear_collection import list_collections, get_collections_info, clear_collection, clear_all_collections



def detect_table_content(content_str: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ í…Œì´ë¸” ë‚´ìš©ì¸ì§€ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    """
    # í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ íŒ¨í„´
    table_patterns = [
        r'\|[^|]+\|[^|]+\|',  # | ì»¬ëŸ¼1 | ì»¬ëŸ¼2 |
        r'[A-Za-z]+\s*:\s*[A-Za-z0-9\s]+,\s*[A-Za-z]+\s*:\s*[A-Za-z0-9\s]+',  # ì»¬ëŸ¼1: ê°’1, ì»¬ëŸ¼2: ê°’2
        r'^\s*[A-Za-z]+\s*:\s*[A-Za-z0-9\s]+\s*$',  # ë‹¨ì¼ ì»¬ëŸ¼: ê°’
    ]
    
    for pattern in table_patterns:
        if re.search(pattern, content_str, re.MULTILINE):
            return True
    
    return False

def calculate_table_match_score(content_str: str, table_data: dict) -> int:
    """
    í…Œì´ë¸” ë§¤ì¹­ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    score = 0
    
    # llm_generate_data ë§¤ì¹­
    if table_data.get('llm_generate_data'):
        for llm_item in table_data['llm_generate_data']:
            if llm_item in content_str:
                score += len(llm_item) * 2  # ë” ê¸´ ë§¤ì¹­ì— ë” ë†’ì€ ì ìˆ˜
    
    # í…Œì´ë¸” êµ¬ì¡° íŒ¨í„´ ë§¤ì¹­
    table_pattern = r'\|[^|]+\|[^|]+\|'
    if re.search(table_pattern, content_str):
        score += 50
    
    # í˜ì´ì§€ ë²ˆí˜¸ ì¼ì¹˜ (ì •ìˆ˜ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ)
    if 'page' in table_data and str(table_data['page']) in content_str:
        score += 100
    
    return score

if __name__ == "__main__" :
    uvicorn.run("app:app", host = "0.0.0.0", port=8000, reload = True)

app = FastAPI()

# CORS í—ˆìš© (í”„ë¡ íŠ¸ì—”ë“œì™€ì˜ í†µì‹ ì„ ìœ„í•´)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ì„œë¹„ìŠ¤ ì‹œì—ëŠ” ë„ë©”ì¸ ì œí•œ ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    question: str

@app.post("/process-pdf")
async def process_pdf_endpoint(files: List[UploadFile] = File(...)):
    logger.info("âœ… /process-pdf ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ ì‹œì‘")
    try:
        logger.info(f"ğŸ“„ ì—…ë¡œë“œëœ íŒŒì¼ ê°œìˆ˜: {len(files)}")
        
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„± (docs í•˜ìœ„)
        upload_dir = os.path.join(BASE_DIR, "docs", "uploads")
        os.makedirs(upload_dir, exist_ok=True)
        
        # ëª¨ë“  íŒŒì¼ì„ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ì— ì €ì¥
        uploaded_files = []
        
        for i, file in enumerate(files):
            logger.info(f"ğŸ“„ íŒŒì¼ {i+1} ì²˜ë¦¬ ì¤‘: {file.filename}")
            
            # íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì¤‘ë³µ ë°©ì§€)
            safe_filename = f"upload_{i+1}_{file.filename.replace(' ', '_')}"
            file_path = os.path.join(upload_dir, safe_filename)
            
            # íŒŒì¼ ì €ì¥
            content = await file.read()
            with open(file_path, "wb") as f:
                f.write(content)
            
            uploaded_files.append(file_path)
            logger.info(f"ğŸ’¾ íŒŒì¼ {i+1} ì €ì¥ ì™„ë£Œ: {file_path}")
        
        logger.info(f"ğŸ’¾ ì´ {len(uploaded_files)}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        logger.info(f"ğŸ“ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬: {upload_dir}")
        logger.info(f"ğŸ“„ ì €ì¥ëœ íŒŒì¼ë“¤: {uploaded_files}")

        # allgeronx_chunker.pyì˜ í•¨ìˆ˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œ
        logger.info(">> 1. PDF -> Markdown ë³€í™˜ ì‹œì‘")
        results, md_files = process_files_in_directory(upload_dir)
        if not results:
            logger.error("âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: 'results'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            raise HTTPException(status_code=400, detail="PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        logger.info("âœ… 1. PDF -> Markdown ë³€í™˜ ì™„ë£Œ")

        # ì²« ë²ˆì§¸ íŒŒì¼ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© (íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°)
        filename_wo_ext = os.path.splitext(os.path.basename(files[0].filename))[0]
        md_file_name = filename_wo_ext

        logger.info(">> 2. ë§ˆí¬ë‹¤ìš´ Chunking ì‹œì‘")
        df = batch_process_directory(md_files[0], md_file_name)  # md_filesëŠ” ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
        if df.empty:
            logger.error("âŒ ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬ ì‹¤íŒ¨: ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            raise HTTPException(status_code=400, detail="ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        logger.info(f"âœ… 2. ë§ˆí¬ë‹¤ìš´ Chunking ì™„ë£Œ. {len(df)}ê°œ íŒ¨ì‹œì§€ ìƒì„±.")

        all_tables_data = []
        for result in results:
            all_tables_data.extend(result['tables_data'])

        logger.info(">> 3. í…Œì´ë¸” ë³‘í•© ì‹œì‘")
        df, all_tables_data = merge_adjacent_table(df, all_tables_data, md_file_name)
        logger.info("âœ… 3. í…Œì´ë¸” ë³‘í•© ì™„ë£Œ")

        logger.info(">> 4. í…Œì´ë¸” ë°ì´í„° ë¬¸ì¥ ë³€í™˜ ì‹œì‘")
        all_tables_data = parse_table_for_embedding(all_tables_data)
        df, all_tables_data = replace_markdown_with_llm_data(df, all_tables_data, md_file_name)
        logger.info("âœ… 4. í…Œì´ë¸” ë°ì´í„° ë¬¸ì¥ ë³€í™˜ ì™„ë£Œ")

        logger.info(">> 5. ëŒ€ìš©ëŸ‰ íŒ¨ì‹œì§€ ë¶„í•  ì‹œì‘")
        df = divide_large_passage(df, md_file_name)
        logger.info("âœ… 5. ëŒ€ìš©ëŸ‰ íŒ¨ì‹œì§€ ë¶„í•  ì™„ë£Œ")


        # process_chunks í•¨ìˆ˜ì— ë§ê²Œ ë°ì´í„° ê°€ê³µ (í…Œì´ë¸” ë©”íƒ€ë°ì´í„° í™œìš©)
        logger.info(">> 6. ì„ë² ë”© ë°ì´í„° ê°€ê³µ ì‹œì‘")
        chunks_for_embedding = []
        for _, row in df.iterrows():
            page = row['pages'][0] if row['pages'] else 1
            
            # processing_pdf.pyì—ì„œ ìƒì„±í•œ íŠœí”Œ êµ¬ì¡°ë¥¼ (text, page) í˜•íƒœë¡œ ë³€í™˜
            if isinstance(row['content'], tuple) and len(row['content']) >= 5:
                # íŠœí”Œì¸ ê²½ìš°: (ëŒ€ì œëª©, ì¤‘ì œëª©, ì†Œì œëª©, ì„¸ì œëª©, ë‚´ìš©, ...)
                content_text = row['content'][4]  # 5ë²ˆì§¸ ìš”ì†Œê°€ ì‹¤ì œ ë‚´ìš©
            else:
                # ë¬¸ìì—´ì¸ ê²½ìš°: ê·¸ëŒ€ë¡œ ì‚¬ìš©
                content_text = row['content']
            
            # í…Œì´ë¸” ë°ì´í„°ì¸ì§€ í™•ì¸í•˜ê³  ë©”íƒ€ë°ì´í„° ë§¤ì¹­ (ê°œì„ ëœ ë²„ì „)
            table_metadata = None
            content_str = str(row['content'])
            
            # ë°©ë²• 1: ê°œì„ ëœ ì ìˆ˜ ê¸°ë°˜ ë§¤ì¹­
            best_match_score = 0
            best_match_table = None
            
            # í…Œì´ë¸” ë‚´ìš©ì¸ì§€ ë¨¼ì € ê°ì§€
            is_table_content = detect_table_content(content_str)
            
            for table in all_tables_data:
                match_score = calculate_table_match_score(content_str, table)
                
                # í˜ì´ì§€ ë²ˆí˜¸ ì¼ì¹˜ ì‹œ ì¶”ê°€ ì ìˆ˜
                if table['page'] == page:
                    match_score += 100
                
                # í…Œì´ë¸” ë‚´ìš©ì´ ê°ì§€ëœ ê²½ìš° ì¶”ê°€ ì ìˆ˜
                if is_table_content:
                    match_score += 200
                
                # ë” ë†’ì€ ì ìˆ˜ì˜ ë§¤ì¹­ ì„ íƒ
                if match_score > best_match_score:
                    best_match_score = match_score
                    best_match_table = table
            
            if best_match_table and best_match_score > 100:  # ìµœì†Œ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
                table_metadata = best_match_table
                logger.info(f"ğŸ“Š í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë§¤ì¹­ (ì ìˆ˜ ê¸°ë°˜): {table_metadata['key']} (í˜ì´ì§€: {table_metadata['page']}, ì ìˆ˜: {best_match_score})")
            
            # ë°©ë²• 2: ë°±ì—… ë§¤ì¹­ (í˜ì´ì§€ ê¸°ë°˜)
            if not table_metadata and is_table_content:
                # í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë§¤ì¹­ (í…Œì´ë¸” ë‚´ìš©ì´ ê°ì§€ëœ ê²½ìš°ì—ë§Œ)
                for table in all_tables_data:
                    if table['page'] == page:
                        table_metadata = table
                        logger.info(f"ğŸ“Š í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë§¤ì¹­ (í˜ì´ì§€ ë°±ì—…): {table['key']} (í˜ì´ì§€: {table['page']})")
                        break
            
            # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…Œì´ë¸” ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì €ì¥
            if table_metadata:
                # í…Œì´ë¸” ì¹´í…Œê³ ë¦¬ ìƒì„± (ì˜ˆ: "Table_5x3", "Table_10x4")
                category = f"Table_{table_metadata['rows']}x{table_metadata['columns']}"
                
                # í…Œì´ë¸” í‚¤ë¥¼ í† í”½ìœ¼ë¡œ ì‚¬ìš© (ê¸¸ì´ ì œí•œ ë° ì •ì œ)
                topic = table_metadata['key'][:200] if len(table_metadata['key']) > 200 else table_metadata['key']
                
                # ë³‘í•©ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                if table_metadata.get('merged_indices'):
                    category += "_Merged"
                    topic = f"Merged_{topic}"
                
                # í…Œì´ë¸” ë°ì´í„°ì¸ì§€ í™•ì¸ (llm_generate_dataê°€ ìˆìœ¼ë©´ í…Œì´ë¸”)
                if table_metadata.get('llm_generate_data'):
                    logger.info(f"ğŸ“Š í…Œì´ë¸” ë°ì´í„° ê°ì§€: {category} - {topic}")
                else:
                    logger.info(f"ğŸ“„ ì¼ë°˜ í…ìŠ¤íŠ¸ (í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ìˆìŒ): {category} - {topic}")
                
                chunks_for_embedding.append((content_text, page, category, topic, table_metadata))
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                category = "Text"
                topic = f"Page_{page}"
                logger.info(f"ğŸ“„ ì¼ë°˜ í…ìŠ¤íŠ¸: {category} - {topic}")
                chunks_for_embedding.append((content_text, page, category, topic, None))
                
        logger.info(f"âœ… 6. ì„ë² ë”© ë°ì´í„° ê°€ê³µ ì™„ë£Œ. {len(chunks_for_embedding)}ê°œ ì²­í¬ ì¤€ë¹„.")

        # ì„ë² ë”© ë° ì €ì¥ (í…Œì´ë¸” ë©”íƒ€ë°ì´í„° í¬í•¨)
        logger.info(">> 7. Milvus ì„ë² ë”© ë° ì €ì¥ ì‹œì‘")
        # PDF ì²˜ë¦¬ ì‹œì—ëŠ” ê¸°ë³¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
        process_chunks_with_metadata(chunks_for_embedding, collection_name="Booking_Embedding")
        logger.info("âœ… 7. Milvus ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ")


        # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì‚­ì œ
        for uploaded_file_path in uploaded_files:
            if os.path.exists(uploaded_file_path):
                os.remove(uploaded_file_path)
                logger.info(f"ğŸ—‘ï¸ ì—…ë¡œë“œ íŒŒì¼ ì‚­ì œ: {uploaded_file_path}")
        
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
        try:
            if os.path.exists(upload_dir) and not os.listdir(upload_dir):
                os.rmdir(upload_dir)
                logger.info(f"ğŸ—‘ï¸ ë¹ˆ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ì‚­ì œ: {upload_dir}")
        except Exception as e:
            logger.warning(f"âš ï¸ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {e}")

        # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆì„ ë•Œ ë°˜í™˜
        logger.info("ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì„±ê³µ! JSON ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return JSONResponse(content={"success": True, "message": "PDF íŒŒì¼ ì²˜ë¦¬ ë° ì„ë² ë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", "chunk_count": len(df)})

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"â—ï¸â—ï¸â—ï¸ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ â—ï¸â—ï¸â—ï¸\n{error_details}")
        return JSONResponse(
            status_code=500,
            content={"error": "ë°±ì—”ë“œ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ", "details": str(e), "traceback": error_details}
        )



@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    answer = handle_rag(req.question)
    return {"answer": answer}


@app.post("/api/v1/agent/excel_rag_generator")
async def excel_rag_generator_endpoint(
    file: UploadFile = File(...),
    collection_name: str = Form(...)
):
    """
    ì—‘ì…€ íŒŒì¼ì„ ë°›ì•„ 'question' ì»¬ëŸ¼ì˜ ê° ì§ˆë¬¸ì— ëŒ€í•œ RAG ë‹µë³€ì„ ìƒì„±í•˜ê³ ,
    'answer' ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì—¬ ìˆ˜ì •ëœ ì—‘ì…€ íŒŒì¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logger.info(f"âœ… /api/v1/agent/excel_rag_generator ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ ì‹œì‘ (ì»¬ë ‰ì…˜: {collection_name})")
    
    # íŒŒì¼ í™•ì¥ì ê²€ì‚¬
    if not file.filename.endswith((".xlsx", ".xls")):
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file.filename}")
        raise HTTPException(status_code=400, detail="ì˜ëª»ëœ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. .xlsx ë˜ëŠ” .xls íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    try:
        # ì—‘ì…€ íŒŒì¼ ì½ê¸°
        content = await file.read()
        df = pd.read_excel(content)
        logger.info(f"ğŸ“„ ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {file.filename}, {len(df)}ê°œ í–‰")

        # 'question' ì»¬ëŸ¼ í™•ì¸
        if 'question' not in df.columns:
            logger.error("âŒ 'question' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise HTTPException(status_code=400, detail="'question' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ë‹µë³€ ìƒì„± (ì„ íƒëœ ì»¬ë ‰ì…˜ ì‚¬ìš©)
        answers = []
        for question in df['question']:
            if pd.isna(question) or not str(question).strip():
                answers.append("") # ì§ˆë¬¸ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°
                continue
            
            logger.info(f"ğŸ’¬ RAG ì—ì´ì „íŠ¸ í˜¸ì¶œ: '{question}' (ì»¬ë ‰ì…˜: {collection_name})")
            answer = handle_rag(str(question), collection_name=collection_name)
            answers.append(answer)
            logger.info("ğŸ’¡ RAG ë‹µë³€ ìˆ˜ì‹ ")

        # ë‹µë³€ ì»¬ëŸ¼ ì¶”ê°€
        df['answer'] = answers
        logger.info("âœ… ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ë° 'answer' ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")

        # ìˆ˜ì •ëœ ì—‘ì…€ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
        output = io.BytesIO()
        df.to_excel(output, index=False)
        output.seek(0)
        
        # ê²°ê³¼ íŒŒì¼ëª… ìƒì„±
        result_filename = f"result_{file.filename}"
        download_directory = os.getenv('DOWNLOAD_DIRECTORY', 'save_result')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.join(download_directory, 'answer'), exist_ok=True)

        # íŒŒì¼ì„ ë””ìŠ¤í¬ì— ì €ì¥
        result_path = os.path.join(download_directory, 'answer', result_filename)
        with open(result_path, 'wb') as f:
            f.write(output.getvalue())
        
        logger.info(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {result_path}")
        
        # JSON ì‘ë‹µ ë°˜í™˜
        return JSONResponse({
            "status": "success",
            "file_name": result_filename,
            "message": "RAG ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        })

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"â—ï¸â—ï¸â—ï¸ ì—‘ì…€ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ â—ï¸â—ï¸â—ï¸\n{error_details}")
        raise HTTPException(
            status_code=500,
            detail=f"ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@app.get("/")
async def root():
    return {"message": "RAG Chatbot API is running!"}


@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "backend"}

# HTML í…œí”Œë¦¿ ì—”ë“œí¬ì¸íŠ¸ë“¤ì€ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì œê±°
# í”„ë¡ íŠ¸ì—”ë“œ: http://localhost:3000 ì—ì„œ ì›¹ í˜ì´ì§€ ì„œë¹™


# í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ Form (PDF, ì´ë¯¸ì§€, XML ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±)
@app.post("/uploadfiles/pdf")
async def create_upload_documents(
    project_id: str = Form(...),
    file1: UploadFile = File(...),
    file2: UploadFile = File(...)
):
    
    async def save_file_to_temp(upload_file: UploadFile):
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        supported_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.xml']
        file_extension = os.path.splitext(upload_file.filename)[1].lower()
        
        if file_extension not in supported_extensions:
            logger.error(f"app.py: File with invalid extension uploaded: {upload_file.filename}")
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}. ì§€ì› í˜•ì‹: {', '.join(supported_extensions)}")
        
        temp_file = NamedTemporaryFile(delete=False, suffix=file_extension)
        with open(temp_file.name, 'wb') as f:
            content = await upload_file.read()
            f.write(content)
        
        logger.info(f"app.py: File saved to temporary location: {temp_file.name}")
        return temp_file.name

    file_path1 = None
    file_path2 = None
    try:
        file_path1 = await save_file_to_temp(file1)
        file_path2 = await save_file_to_temp(file2)

        result = auto_generator.pdf_question_generator.generate_questions_from_documents(
            file_path1=file_path1,
            file_path2=file_path2,
            project_id=project_id
        )

        if not result or 'file_name' not in result:
            logger.error(f"app.py: Question generation failed or returned an invalid result: {result}")
            raise HTTPException(status_code=500, detail="ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"app.py: Successfully generated questions. Result: {result}")
        return result

    except ValueError as e:
        logger.exception("app.py: A value error occurred during document processing")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.exception("app.py: An unexpected error occurred during document processing")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if file_path1 and os.path.exists(file_path1):
            os.remove(file_path1)
            logger.info(f"app.py: Temporary file {file_path1} deleted.")
        if file_path2 and os.path.exists(file_path2):
            os.remove(file_path2)
            logger.info(f"app.py: Temporary file {file_path2} deleted.")

# í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ Form
@app.post("/uploadfiles/sentence")
async def create_upload_files(
    field_business: Union[str, None] = Form(None),
    user_type: Union[str, None] = Form(None),
    Constraints: Union[str, None] = Form(None),
    project_id: str = Form(...),
    file1: UploadFile = File(...),
    file2: UploadFile = File(...)
):
    try:
        # Azure ì„¤ì • ì´ˆê¸°í™”
        auto_generator.sentence_generator.initialize_azure_config(project_id)
        
        # íŒŒì¼ ì €ì¥ í•¨ìˆ˜
        async def save_file_to_temp(upload_file: UploadFile):
            if not upload_file.filename.endswith(".xlsx"):
                logger.error(f"app.py: File with invalid extension uploaded: {upload_file.filename}")
                raise ValueError(f"Invalid file extension for {upload_file.filename}")
            temp_file = NamedTemporaryFile(delete=False, suffix=".xlsx")
            with open(temp_file.name, 'wb') as f:
                while True:
                    chunk = await upload_file.read(1024 * 1024)  # 1MB chunks
                    if not chunk:
                        break
                    f.write(chunk)
            logger.info(f"app.py: File saved to temporary location: {temp_file.name}")
            return temp_file.name

        # Q&A Set(ì§ˆë‹µì…‹) ì €ì¥
        file_path = await save_file_to_temp(file1)
        
        # Entity Dict(ì—”í‹°í‹° ì‚¬ì „) ì €ì¥
        file_path2 = await save_file_to_temp(file2)

        # ë°ì´í„° í”„ë ˆì„ ë¡œë“œ ë° ê²€ì¦
        df = pd.read_excel(file_path)
        edf = pd.read_excel(file_path2)
        
        if (df.columns[0] != 'Answer') and (edf.columns[0] != 'Entry'):
            logger.warning("app.py: File contents do not meet the expected format")
            raise ValueError("File contents do not meet expected format")

        # ë°ì´í„° ì €ì¥ (project_id ì œê±°)
        result = auto_generator.sentence_generator.Save_data(field_business, user_type, df, edf, Constraints)
        logger.info(f"app.py: Save_data result: {result}")
        if not result or 'file_name' not in result:
            logger.error(f"app.py: Returned result does not contain file_name! result: {result}")
        logger.info("app.py: Data successfully saved.")
        return result

    except ValueError as e:
        logger.exception("app.py: A value error occurred")
        return {"error": str(e)}
    except Exception as e:
        logger.exception("app.py: An unexpected error occurred")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if 'file_path' in locals() and os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"app.py: Temporary file {file_path} deleted.")
        if 'file_path2' in locals() and os.path.exists(file_path2):
            os.remove(file_path2)
            logger.info(f"app.py: Temporary file {file_path2} deleted.")


# í•™ìŠµë°ì´í„° ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
@app.get("/downloadfiles/{folder}/{filename}/")
def download_files(folder: str, filename: str):
    logger.info(f"app.py: Download request received for file: {filename} in folder: {folder}")
    
    # í—ˆìš©ëœ í´ë” ëª©ë¡ìœ¼ë¡œ ë³´ì•ˆ ê°•í™”
    allowed_folders = ['sentence', 'pdf_questions']
    if folder not in allowed_folders:
        logger.error(f"app.py: Forbidden folder access attempt: {folder}")
        raise HTTPException(status_code=403, detail="Forbidden folder")

    base_directory = os.getenv('DOWNLOAD_DIRECTORY', f'save_result/{folder}')
    # ê²½ë¡œ ì¡°ì‘ ê³µê²© ë°©ì§€
    safe_filename = os.path.basename(filename)
    file_path = os.path.join(base_directory, safe_filename)
    
    logger.info(f"app.py: Attempting to download file from path: {file_path}")
    
    if not os.path.isfile(file_path):
        # ë§Œì•½ ê¸°ë³¸ ê²½ë¡œì— íŒŒì¼ì´ ì—†ë‹¤ë©´, í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì¬ì‹œë„
        # (save_resultê°€ ì•„ë‹Œ docs/save_result ë“±ì— ì €ì¥ë˜ëŠ” ê²½ìš° ëŒ€ë¹„)
        alt_base_directory = os.path.join('docs', 'save_result', folder)
        file_path = os.path.join(alt_base_directory, safe_filename)
        logger.info(f"app.py: File not in primary path, trying alternative: {file_path}")
        if not os.path.isfile(file_path):
             logger.error(f"app.py: File not found at path: {file_path}")
             raise HTTPException(status_code=404, detail="File not found")

    return FileResponse(file_path, media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', filename=filename)


# ì‚¬ì—…ë¶€ ë“œë¡­ë‹¤ìš´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
@app.get("/get-business-department")
async def get_dropdown_business_department():
    file_path = "/app/docs/public/business_department.xlsx"
    df = pd.read_excel(file_path, sheet_name="Sheet1")

    projects = df[["ProjectID", "ProjectName"]].dropna().to_dict(orient="records")

    return {"options": projects}

# ë¡œê·¸ ë°ì´í„° ë°˜í™˜ API
@app.get("/log")
async def get_logs():
    try:
        log_file_path = "/app/logs/debug.log"
        
        if not os.path.exists(log_file_path):
            logger.warning(f"app.py: Log file not found: {log_file_path}")
            return {"logs": [], "error": "Log file not found"}
        
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()
        
        # ìµœê·¼ 500ì¤„ ë°˜í™˜ (ë” ë§ì€ ë¡œê·¸ í‘œì‹œ)
        recent_logs = log_lines[-500:] if len(log_lines) > 500 else log_lines
        
        # ì‹¤ì œ ë¡œê·¸ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ìš”í•œ ë¡œê·¸ë§Œ í•„í„°ë§)
        valid_logs = []
        for line in recent_logs:
            line = line.strip()
            if line and not line.startswith("returning"):
                # HTTP ë””ë²„ê·¸ ë¡œê·¸ ì œì™¸í•˜ê³  ì¤‘ìš”í•œ ë¡œê·¸ë§Œ í¬í•¨
                if any(keyword in line for keyword in [
                    "INFO", "ERROR", "WARNING", "SUCCESS", "FAILED", 
                    "Milvus", "Azure", "RAG", "Question", "File", "Upload"
                ]):
                    valid_logs.append(line)
        
        # ë¡œê·¸ API í˜¸ì¶œ ë©”ì‹œì§€ëŠ” DEBUG ë ˆë²¨ë¡œ ë³€ê²½ (í™”ë©´ì— í‘œì‹œë˜ì§€ ì•ŠìŒ)
        logger.debug(f"app.py: Log data requested, returning {len(valid_logs)} filtered log lines")
        return {"logs": valid_logs}
        
    except Exception as e:
        logger.error(f"app.py: Error reading log file: {e}")
        return {"logs": [], "error": str(e)}

# mcp ì—°ë™ í…ŒìŠ¤íŠ¸ API
@app.get("/api/mcp")
async def get_mcp_data():
    mcp_file_path = os.path.join(os.path.dirname(__file__), ".gemini", "mcp_test.json")
    try:
        with open(mcp_file_path, 'r', encoding='utf-8') as f:
            mcp_data = json.load(f)
        logger.info(f"app.py: Successfully loaded MCP data from {mcp_file_path}")
        return JSONResponse(content=mcp_data)
    except FileNotFoundError:
        logger.error(f"app.py: MCP data file not found at: {mcp_file_path}")
        raise HTTPException(status_code=404, detail="MCP data file not found")
    except Exception as e:
        logger.exception("app.py: An error occurred while processing MCP data")
        raise HTTPException(status_code=500, detail=str(e))

# ì»¬ë ‰ì…˜ ê´€ë¦¬ API
@app.get("/api/collections")
async def get_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” API"""
    try:
        collections = list_collections()
        return JSONResponse(content={"collections": collections})
    except Exception as e:
        logger.error(f"âŒ ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/collections/info")
async def get_collections_info():
    """ì»¬ë ‰ì…˜ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” API"""
    try:
        collections_info = get_collections_info()
        return JSONResponse(content={"collections_info": collections_info})
    except Exception as e:
        logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/collections/{collection_name}")
async def delete_collection(collection_name: str):
    """íŠ¹ì • ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ëŠ” API"""
    try:
        clear_collection(collection_name)
        return JSONResponse(content={"success": True, "message": f"ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ ì™„ë£Œ"})
    except Exception as e:
        logger.error(f"âŒ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/collections")
async def delete_all_collections():
    """ëª¨ë“  ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ëŠ” API"""
    try:
        clear_all_collections()
        return JSONResponse(content={"success": True, "message": "ëª¨ë“  ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ"})
    except Exception as e:
        logger.error(f"âŒ ëª¨ë“  ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/download/excel-rag/{filename}")
async def download_excel_rag(filename: str):
    """
    Excel RAG ê²°ê³¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
    """
    try:
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        download_directory = os.getenv('DOWNLOAD_DIRECTORY', 'save_result')
        file_path = os.path.join(download_directory, 'answer', filename)
        
        logger.info(f"ğŸ“¥ Excel RAG íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìš”ì²­: {filename}")
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(file_path):
            logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ
        return FileResponse(
            file_path, 
            media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            filename=filename
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Excel RAG íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")